#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created on Fri Aug 18, 2023@author: xychen"""import osimport mathimport torchimport randomimport pickleimport numpy as npimport SimpleITK as sitkfrom torch.utils import datadef resample_img_lab_pair(itk_image, itk_label, itk_mask=None, out_spacing=[1.0, 1.0, 3.0], out_size=None, rotate=True, views=['axial']):        imageFilter = sitk.MinimumMaximumImageFilter()    imageFilter.Execute(itk_image)        width, height, depth = itk_image.GetSize()    original_spacing = itk_image.GetSpacing()        rotation_center = itk_image.TransformIndexToPhysicalPoint((int(np.ceil(width/2)),                                                                int(np.ceil(height/2)),                                                                int(np.ceil(depth/2))))    if rotate:        theta_z = random.uniform(-math.pi/12.0, math.pi/12.0)        theta_y = random.uniform(-math.pi/12.0, math.pi/12.0)        theta_x = random.uniform(-math.pi/12.0, math.pi/12.0)    else:        """To ensure correctness, we only rotate a sparsely annotated image along one of the directions of annotation"""        if len(views) > 1:            view = random.choice(views)        if view == 'axial':            theta_z, theta_y, theta_x = random.uniform(-math.pi/12.0, math.pi/12.0), 0, 0        elif view == 'sagittal':            theta_z, theta_y, theta_x = 0, random.uniform(-math.pi/12.0, math.pi/12.0), 0        else:            theta_z, theta_y, theta_x = 0, 0, random.uniform(-math.pi/12.0, math.pi/12.0)        translation = [0, 0, 0]        scale = random.uniform(0.9, 1.1)    if rotate:        scale_factor = 3 * [scale,]    else:        """To ensure correctness, only in-slice resizing is done for sparsely annotated images"""        if view == 'axial':            scale_factor = [scale, scale, 1.0] # In the order of x, y, z        elif view == 'sagittal':            scale_factor = [scale, 1.0, scale] # In the order of x, y, z        else:            scale_factor = [1.0, scale, scale] # In the order of x, y, z        similarity = sitk.Euler3DTransform(rotation_center, theta_x, theta_y, theta_z, translation)        T=sitk.AffineTransform(3)    T.SetMatrix(similarity.GetMatrix())    T.SetCenter(similarity.GetCenter())    T.SetTranslation(similarity.GetTranslation())    T.Scale(scale_factor)        # Resample images to out_spacing with SimpleITK    original_spacing = itk_image.GetSpacing()    original_size = itk_image.GetSize()        if out_size == None: # This is a work-around        out_size = [            int(np.round(original_size[0] * (original_spacing[0] / out_spacing[0]) * scale_factor[0])),            int(np.round(original_size[1] * (original_spacing[1] / out_spacing[1]) * scale_factor[1])),            int(np.round(original_size[2] * (original_spacing[2] / out_spacing[2]) * scale_factor[2]))]        out_spacing = [out_spacing[0] / scale_factor[0], out_spacing[1] / scale_factor[1], out_spacing[2] / scale_factor[2]]        resample1 = sitk.ResampleImageFilter()    resample1.SetOutputSpacing(out_spacing)    resample1.SetSize(out_size)    resample1.SetOutputDirection(itk_image.GetDirection())    resample1.SetOutputOrigin(itk_image.GetOrigin())    resample1.SetTransform(T)        resample2 = sitk.ResampleImageFilter()    resample2.SetOutputSpacing(out_spacing)    resample2.SetSize(out_size)    resample2.SetOutputDirection(itk_image.GetDirection())    resample2.SetOutputOrigin(itk_image.GetOrigin())    resample2.SetTransform(T)        resample1.SetDefaultPixelValue(imageFilter.GetMinimum())    resample1.SetInterpolator(sitk.sitkLinear)        resample2.SetDefaultPixelValue(0)    resample2.SetInterpolator(sitk.sitkNearestNeighbor)        resampled_image = resample1.Execute(itk_image)    resampled_label = resample2.Execute(itk_label)        if itk_mask == None:        return resampled_image, resampled_label    else:        """We need to handle 'mask' when itk_mask is not None"""        resample3 = sitk.ResampleImageFilter()        resample3.SetOutputSpacing(out_spacing)        resample3.SetSize(out_size)        resample3.SetOutputDirection(itk_image.GetDirection())        resample3.SetOutputOrigin(itk_image.GetOrigin())        resample3.SetTransform(T)                resample3.SetDefaultPixelValue(1)        resample3.SetInterpolator(sitk.sitkNearestNeighbor)                resampled_mask = resample3.Execute(itk_mask)                return resampled_image, resampled_label, resampled_maskclass CustomDataSet(data.Dataset):    def __init__(self, args, samples_with_replacement_per_modality, device_id):        self.args = args        self.partial_tree_repr_path = args.partial_tree_representation        self.sparse_tree_repr_path = args.sparse_tree_representation        self.root = args.data_dir        self.total_classes = args.total_classes        self.samples_with_replacement_per_modality = samples_with_replacement_per_modality        self.device_id = device_id        self.crop_d, self.crop_h, self.crop_w = args.input_size        self.spacing = args.common_spacing        self.datasets_for_training = args.datasets_for_training        self.modalities_for_training = args.modalities_for_training                with open(self.partial_tree_repr_path, 'rb') as f:            self.partial_tree_repr = pickle.load(f)                with open(self.sparse_tree_repr_path, 'rb') as f:            self.sparse_tree_repr = pickle.load(f)                self.files = os.listdir(self.root + "/imagesTr")                self.modality2datasets = {}        for train_subject in self.files:            modality = train_subject.split('_')[1]            dataset = train_subject.split('_')[0]                        if dataset not in self.datasets_for_training or modality not in self.modalities_for_training:                continue                        if modality not in self.modality2datasets.keys():                self.modality2datasets[modality] = {}            if dataset not in self.modality2datasets[modality]:                self.modality2datasets[modality][dataset] = []            self.modality2datasets[modality][dataset].append(train_subject)                self.modalities = sorted(list(self.modality2datasets.keys()))                if self.device_id == 0:            for modality in self.modality2datasets.keys():                print("The dataset includes following ", modality.upper(), " images")                for dataset in self.modality2datasets[modality].keys():                    print(dataset, "(", len(self.modality2datasets[modality][dataset]), "subjects for training", ")")        def __len__(self):        return len(self.modality2datasets) * self.samples_with_replacement_per_modality        def truncate_CT(self, CT):                CT = np.clip(CT, self.args.clip_lower, self.args.clip_upper)        CT = (CT - self.args.clip_lower) / (self.args.clip_upper - self.args.clip_lower)        return CT        def truncate_MR(self, MR):                mri_min = np.quantile(MR[np.where(MR>0)], 0.01)        mri_max = np.quantile(MR[np.where(MR>0)], 0.99)                MR = np.clip(MR, mri_min, mri_max)        MR = (MR - mri_min) / (mri_max - mri_min)        return MR        def corrected_crop(self, array, image_size):        array_ = array.copy()        image_size_ = image_size.copy()                copy_from = [0, 0, 0, 0, 0, 0]        copy_to = [0, 0, 0, 0, 0, 0]        ## 0 ##        if array[0] < 0:            copy_from[0] = 0            copy_to[0] = int(abs(array_[0]))        else:            copy_from[0] = int(array_[0])            copy_to[0] = 0        ## 1 ##        if array[1] > image_size_[0]:            copy_from[1] = None            copy_to[1] = -int(array_[1] - image_size_[0])        else:            copy_from[1] = int(array_[1])            copy_to[1] = None        ## 2 ##        if array[2] < 0:            copy_from[2] = 0            copy_to[2] = int(abs(array_[2]))        else:            copy_from[2] = int(array_[2])            copy_to[2] = 0        ## 3 ##        if array[3] > image_size_[1]:            copy_from[3] = None            copy_to[3] = -int(array_[3] - image_size_[1])        else:            copy_from[3] = int(array_[3])            copy_to[3] = None        ## 4 ##        if array[4] < 0:            copy_from[4] = 0            copy_to[4] = int(abs(array_[4]))        else:            copy_from[4] = int(array_[4])            copy_to[4] = 0        ## 5 ##          if array[5] > image_size_[2]:            copy_from[5] = None            copy_to[5] = -int(array_[5] - image_size_[2])        else:            copy_from[5] = int(array_[5])            copy_to[5] = None        return copy_from, copy_to        def crop_patch_from_given_fg(self, image, label, mask, label_i, annotation_ids, annotated_categories_along_z_axis, annotated_categories_along_y_axis, annotated_categories_along_x_axis, patch_size):                image_shape = image.shape        label_shape = label.shape        assert np.all(image_shape == label_shape)                if label_i in annotation_ids:            where = np.where(label == label_i)        else: # This occurs infrequently, typically when small structures are omitted during the resampling process.            label_i = random.choice(annotation_ids)            where = np.where(label == label_i)                depth_indicies = where[0]        height_indicies = where[1]        width_indicies = where[2]                assert depth_indicies.shape[0] > 0                num_voxels = depth_indicies.shape[0]        idx = random.randint(0, num_voxels-1)                center_z = depth_indicies[idx]        center_y = height_indicies[idx]        center_x = width_indicies[idx]                center = np.array([center_z-int(patch_size[0]//2), center_z+int(patch_size[0]//2),                           center_y-int(patch_size[1]//2), center_y+int(patch_size[1]//2),                           center_x-int(patch_size[2]//2), center_x+int(patch_size[2]//2)])                copy_from, copy_to = self.corrected_crop(center, np.array(image.shape))                cf_z_lower_bound = int(copy_from[0])        if copy_from[1] is not None:            cf_z_higher_bound = int(copy_from[1])        else:            cf_z_higher_bound = None                cf_y_lower_bound = int(copy_from[2])        if copy_from[3] is not None:            cf_y_higher_bound = int(copy_from[3])        else:            cf_y_higher_bound = None                cf_x_lower_bound = int(copy_from[4])        if copy_from[5] is not None:            cf_x_higher_bound = int(copy_from[5])        else:            cf_x_higher_bound = None                image_patch = np.zeros([patch_size[0], patch_size[1], patch_size[2]], dtype=np.float32)        label_patch = np.zeros([patch_size[0], patch_size[1], patch_size[2]], dtype=np.int32)        mask_patch = np.ones([patch_size[0], patch_size[1], patch_size[2]], dtype=np.int32)        annotated_categories_z_axis = np.zeros([patch_size[0], self.total_classes], dtype=np.int32)        annotated_categories_y_axis = np.zeros([patch_size[1], self.total_classes], dtype=np.int32)        annotated_categories_x_axis = np.zeros([patch_size[2], self.total_classes], dtype=np.int32)                image_patch[int(copy_to[0]):copy_to[1],                    int(copy_to[2]):copy_to[3],                    int(copy_to[4]):copy_to[5]] = \                                                      image[cf_z_lower_bound:cf_z_higher_bound,                                                            cf_y_lower_bound:cf_y_higher_bound,                                                            cf_x_lower_bound:cf_x_higher_bound]                label_patch[int(copy_to[0]):copy_to[1],                    int(copy_to[2]):copy_to[3],                    int(copy_to[4]):copy_to[5]] = \                                                      label[cf_z_lower_bound:cf_z_higher_bound,                                                            cf_y_lower_bound:cf_y_higher_bound,                                                            cf_x_lower_bound:cf_x_higher_bound]                mask_patch[int(copy_to[0]):copy_to[1],                   int(copy_to[2]):copy_to[3],                   int(copy_to[4]):copy_to[5]]  = \                                                      mask[cf_z_lower_bound:cf_z_higher_bound,                                                           cf_y_lower_bound:cf_y_higher_bound,                                                           cf_x_lower_bound:cf_x_higher_bound]                annotated_categories_z_axis[int(copy_to[0]):copy_to[1], :] = annotated_categories_along_z_axis[cf_z_lower_bound:cf_z_higher_bound, :]        annotated_categories_y_axis[int(copy_to[2]):copy_to[3], :] = annotated_categories_along_y_axis[cf_y_lower_bound:cf_y_higher_bound, :]        annotated_categories_x_axis[int(copy_to[4]):copy_to[5], :] = annotated_categories_along_x_axis[cf_x_lower_bound:cf_x_higher_bound, :]                return image_patch.copy(), label_patch.copy(), mask_patch.copy(), annotated_categories_z_axis.copy(), annotated_categories_y_axis.copy(), annotated_categories_x_axis.copy()        def flip_img_lab_array_pair(self, img_array, lab_array, mask_array, annotated_categories_along_z_axis, annotated_categories_along_y_axis, annotated_categories_along_x_axis, direction):        if direction[0] < 0:            img_array = img_array[:, :, ::-1]            lab_array = lab_array[:, :, ::-1]            mask_array = mask_array[:, :, ::-1]            annotated_categories_along_x_axis = annotated_categories_along_x_axis[::-1, :]        if direction[4] < 0:            img_array = img_array[:, ::-1, :]            lab_array = lab_array[:, ::-1, :]            mask_array = mask_array[:, ::-1, :]            annotated_categories_along_y_axis = annotated_categories_along_y_axis[::-1, :]        if direction[8] < 0:            img_array = img_array[::-1, :, :]            lab_array = lab_array[::-1, :, :]            mask_array = mask_array[::-1, :, :]            annotated_categories_along_z_axis = annotated_categories_along_z_axis[::-1, :]        return img_array.copy(), lab_array.copy(), mask_array.copy(), annotated_categories_along_z_axis.copy(), annotated_categories_along_y_axis.copy(), annotated_categories_along_x_axis.copy()        def __getitem__(self, index):                if self.args.train_mode == "mixed":            """ mixed training <=> both sparsely labled data and partially labeled data take up 50% of training samples"""            if index % 2 == 0:                is_sparse_label = [0]                category_selected = random.choice(list(self.partial_tree_repr.keys()))                modality_selected = random.choice(list(self.partial_tree_repr[category_selected].keys()))                dataset_selected = random.choice(list(self.partial_tree_repr[category_selected][modality_selected].keys()))                file = random.choice(self.partial_tree_repr[category_selected][modality_selected][dataset_selected])            else:                is_sparse_label = [1]                category_selected = random.choice(list(self.sparse_tree_repr.keys()))                modality_selected = random.choice(list(self.sparse_tree_repr[category_selected].keys()))                dataset_selected = random.choice(list(self.sparse_tree_repr[category_selected][modality_selected].keys()))                file = random.choice(self.sparse_tree_repr[category_selected][modality_selected][dataset_selected])        elif self.args.train_mode == "partial_only":            is_sparse_label = [0]            category_selected = random.choice(list(self.partial_tree_repr.keys()))            modality_selected = random.choice(list(self.partial_tree_repr[category_selected].keys()))            dataset_selected = random.choice(list(self.partial_tree_repr[category_selected][modality_selected].keys()))            file = random.choice(self.partial_tree_repr[category_selected][modality_selected][dataset_selected])        else:            is_sparse_label = [1]            category_selected = random.choice(list(self.sparse_tree_repr.keys()))            modality_selected = random.choice(list(self.sparse_tree_repr[category_selected].keys()))            dataset_selected = random.choice(list(self.sparse_tree_repr[category_selected][modality_selected].keys()))            file = random.choice(self.sparse_tree_repr[category_selected][modality_selected][dataset_selected])                modality = file.split('_')[1].lower()                imageNII = sitk.ReadImage(os.path.join(self.root, "imagesTr", file))        labelNII = sitk.ReadImage(os.path.join(self.root, "labelsTr", file))                """This code snippet is used to simulate sparsely annotated data using partially (or even fully) annotated data"""        if self.args.use_simulated_sparse_data == True and is_sparse_label[0] == 1:            spacing = imageNII.GetSpacing()            orientation = imageNII.GetDirection()            origin = imageNII.GetOrigin()                        """Note that we simulate with original data, rather than data after resampling"""            labelArray = sitk.GetArrayFromImage(labelNII)                        mask = np.zeros_like(labelArray, dtype=np.int32)            label_shape = labelArray.shape                        """Keep slices in one, two or all 3 directions, depending on what are included in args.views"""            """mask is used to determine if a slice should contribute to loss"""            if "axial" in self.args.views:                for slice_id in range(label_shape[0]):                    if slice_id > 0 and slice_id % self.args.keep_one_every_this_num_of_slices == 0:                        mask[slice_id:slice_id+1, :, :] = 1            if "sagittal" in self.args.views:                for slice_id in range(label_shape[1]):                    if slice_id > 0 and slice_id % self.args.keep_one_every_this_num_of_slices == 0:                        mask[:, slice_id:slice_id+1, :] = 1            if "coronal" in self.args.views:                for slice_id in range(label_shape[2]):                    if slice_id > 0 and slice_id % self.args.keep_one_every_this_num_of_slices == 0:                        mask[:, :, slice_id:slice_id+1] = 1                        labelArray = labelArray * mask                        labelNII = sitk.GetImageFromArray(labelArray)            labelNII.SetSpacing(spacing)            labelNII.SetDirection(orientation)            labelNII.SetOrigin(origin)                        maskNII = sitk.GetImageFromArray(mask)            maskNII.SetSpacing(spacing)            maskNII.SetDirection(orientation)            maskNII.SetOrigin(origin)                """If use_simulated_sparse_data is False, we don't need to simulate"""        if self.args.use_simulated_sparse_data == False and is_sparse_label[0] == 1:            spacing = imageNII.GetSpacing()            orientation = imageNII.GetDirection()            origin = imageNII.GetOrigin()                        labelArray = sitk.GetArrayFromImage(labelNII)                        mask = np.zeros_like(labelArray, dtype=np.int32)            label_shape = labelArray.shape                        """mask is used to determine if a slice should contribute to loss"""            """We assume that for a strucrure with annotation in a slice, all pixels belonging to it have been annotated"""            for slice_id in range(label_shape[0]):                if np.sum(labelArray[slice_id:slice_id+1, :, :]) != 0:                    mask[slice_id:slice_id+1, :, :] = 1                         for slice_id in range(label_shape[1]):                if np.sum(labelArray[:, slice_id:slice_id+1, :]) != 0:                    mask[:, slice_id:slice_id+1, :] = 1                        for slice_id in range(label_shape[2]):                if np.sum(labelArray[:, :, slice_id:slice_id+1]) != 0:                    mask[:, :, slice_id:slice_id+1] = 1                        maskNII = sitk.GetImageFromArray(mask)            maskNII.SetSpacing(spacing)            maskNII.SetDirection(orientation)            maskNII.SetOrigin(origin)                """For sparsely annotated data, we should take care of 'mask'. 'mask' is used for loss computation."""        if is_sparse_label[0] == 0:            image, label = resample_img_lab_pair(imageNII, labelNII, out_spacing=[self.spacing[0], self.spacing[1], self.spacing[2]], rotate=True)        else:            image, label, mask = resample_img_lab_pair(imageNII, labelNII, itk_mask=maskNII, out_spacing=[self.spacing[0], self.spacing[1], self.spacing[2]], rotate=False, views=self.args.views)                image = sitk.GetArrayFromImage(image)        label = np.round(sitk.GetArrayFromImage(label)).astype(np.int32)                if is_sparse_label[0] == 0:            mask = np.ones_like(label, dtype=np.int32)        else:            mask = sitk.GetArrayFromImage(mask)                """Store the information about annotated categories in each slice into the corresponding array"""        """Keep in mind that we assume that, for a strucrure with annotation in a slice (for sparse labels) or volume (for partial labels), all pixels/voxels belonging to it should be annotated"""        num_slices = label.shape[0]        annotated_categories_along_z_axis = np.zeros([num_slices, self.total_classes], dtype=np.int32)        if is_sparse_label[0] == 1:            for slice_id in range(num_slices):                label_slice = label[slice_id:slice_id+1, :, :]                annotated_categories_slice = np.sort(np.unique(label_slice))                annotated_categories_slice = annotated_categories_slice[np.where(annotated_categories_slice != 0)]                for annotated_category in annotated_categories_slice:                    annotated_categories_along_z_axis[slice_id][annotated_category] = 1                num_slices = label.shape[1]        annotated_categories_along_y_axis = np.zeros([num_slices, self.total_classes], dtype=np.int32)        if is_sparse_label[0] == 1:            for slice_id in range(num_slices):                label_slice = label[:, slice_id:slice_id+1, :]                annotated_categories_slice = np.sort(np.unique(label_slice))                annotated_categories_slice = annotated_categories_slice[np.where(annotated_categories_slice != 0)]                for annotated_category in annotated_categories_slice:                    annotated_categories_along_y_axis[slice_id][annotated_category] = 1                num_slices = label.shape[2]        annotated_categories_along_x_axis = np.zeros([num_slices, self.total_classes], dtype=np.int32)        if is_sparse_label[0] == 1:            for slice_id in range(num_slices):                label_slice = label[:, :, slice_id:slice_id+1]                annotated_categories_slice = np.sort(np.unique(label_slice))                annotated_categories_slice = annotated_categories_slice[np.where(annotated_categories_slice != 0)]                for annotated_category in annotated_categories_slice:                    annotated_categories_along_x_axis[slice_id][annotated_category] = 1                image, label, mask, annotated_categories_along_z_axis, annotated_categories_along_y_axis, annotated_categories_along_x_axis = self.flip_img_lab_array_pair(image.copy(), label.copy(), mask.copy(), annotated_categories_along_z_axis, annotated_categories_along_y_axis, annotated_categories_along_x_axis, imageNII.GetDirection())                if modality == "ct":            image = self.truncate_CT(image)        elif modality == "t1w" or modality == "t2w":            image = self.truncate_MR(image)        else:            assert 0, "not defined yet"                """Note some classes with only less than about a hundred voxels can be missing after resampling"""         """annotated_fg_categories is useful only for partially labeled data"""        """annotated_categories_z_axis, annotated_categories_y_axis and annotated_categories_x_axis are useful only for sparsely labeled data"""        annotated_categories = np.sort(np.unique(label))        image_patch, label_patch, mask_patch, annotated_categories_z_axis, annotated_categories_y_axis, annotated_categories_x_axis = self.crop_patch_from_given_fg(image, label, mask, category_selected, annotated_categories, annotated_categories_along_z_axis, annotated_categories_along_y_axis, annotated_categories_along_x_axis, [self.crop_d, self.crop_h, self.crop_w])                if is_sparse_label[0] == 0:            annotated_fg_categories = annotated_categories[np.where(annotated_categories != 0)]        else:            annotated_fg_categories = np.array([])                """Pad num_annotated_fg_categories to have the same length"""        num_annotated_fg_categories = annotated_fg_categories.shape[0]        if num_annotated_fg_categories < self.total_classes:            annotated_fg_categories = np.concatenate([annotated_fg_categories, np.array([-1,] * (self.total_classes - num_annotated_fg_categories))], axis=0)                image_patch = image_patch[np.newaxis, :]                return torch.tensor(image_patch), torch.tensor(label_patch), torch.tensor(annotated_fg_categories), torch.tensor(annotated_categories_z_axis), torch.tensor(annotated_categories_y_axis), torch.tensor(annotated_categories_x_axis), torch.tensor(mask_patch), torch.tensor(is_sparse_label)